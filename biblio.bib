@article{pereira_benchmarking_2018,
  title = {Benchmarking {{Pub}}/{{Sub IoT}} Middleware Platforms for Smart Services},
  issn = {2199-4668, 2199-4676},
  doi = {10.1007/s40860-018-0056-3},
  abstract = {Middleware is being extensively used in Internet of Things (IoT) deployments and is available in a variety of flavors. Despite this extensive use and diversity, a fair comparison of the benefits, disadvantages, and performance of each middleware platform is missing. This comparison is relevant to support the decision process for IoT infrastructure. In this paper, we propose a set of qualitative and quantitative dimensions for benchmarking IoT middleware. We use the publication\textendash{}subscription of a large dataset as use case inspired by a smart city scenario to compare two middleware platforms with standard ambition: FIWARE and oneM2M. We take these metrics and use case and systematically compare the two middleware platforms in the wild. We identify inefficiencies in implementations and characterize performance variations throughout the day, showing that the metrics may also be used for monitoring. Furthermore, we apply the same metrics and use case to two brokers set up in a controlled environment, providing infrastructure- and networking-independent insights. Finally, we summarize useful practical know-how acquired in the process that can speed up entrance into the topic and avoid configuration and implementation pitfalls that impact performance.},
  language = {en},
  journal = {Journal of Reliable Intelligent Environments},
  author = {Pereira, Carlos and Cardoso, Jo{\~a}o and Aguiar, Ana and Morla, Ricardo},
  month = feb,
  year = {2018},
  pages = {1--13},
  file = {/home/luis/Zotero/storage/9BAUCS8M/Pereira et al. - 2018 - Benchmarking PubSub IoT middleware platforms for .pdf;/home/luis/Zotero/storage/2D2IYUUT/s40860-018-0056-3.html}
}



@inproceedings{cardoso_benchmarking_2017,
  title = {Benchmarking {{IoT}} Middleware Platforms},
  doi = {10.1109/WoWMoM.2017.7974339},
  abstract = {Middleware is being extensively used in Internet of Things (IoT) deployments and is available in a variety of flavors - from general-purpose community-driven middleware and telco-developed Machine-to-Machine (M2M) middleware to middleware targeting specific deployments. Despite this extensive use and diversity, little is known about the benefits, disadvantages, and performance of each middleware platform and how the different platforms compare with each other. This comparison is especially relevant to help the design and dimensioning of IoT infrastructure. In this paper, we propose a set of qualitative dimensions and quantitative metrics that can be used for bench-marking IoT middleware. We use the publication-subscription of a large dataset as use case inspired by a smart city scenario to compare two middleware platforms. The methodology enables us to systematically compare the two middleware platforms. Further, we are able to use our approach to identify inefficiencies in implementations and to characterize performance variations throughout the day, showing that the metrics may also be used for monitoring.},
  booktitle = {2017 {{IEEE}} 18th {{International Symposium}} on {{A World}} of {{Wireless}}, {{Mobile}} and {{Multimedia Networks}} ({{WoWMoM}})},
  author = {Cardoso, J. and Pereira, C. and Aguiar, A. and Morla, R.},
  month = jun,
  year = {2017},
  keywords = {Internet of Things,middleware,Middleware,benchmark testing,Context modeling,IoT middleware platforms benchmarking,Machine-to-machine communications,Measurement,Publish-subscribe,qualitative dimensions,quantitative metrics,Sensors,software metrics,software performance evaluation,Standards},
  pages = {1--7},
  file = {/home/luis/Zotero/storage/QFIIBN4W/Cardoso et al. - 2017 - Benchmarking IoT middleware platforms.pdf;/home/luis/Zotero/storage/2MGUT5Q2/7974339.html}
}

@article{razzaque_middleware_2016,
	title = {Middleware for {Internet} of {Things}: {A} {Survey}},
	volume = {3},
	issn = {2327-4662},
	shorttitle = {Middleware for {Internet} of {Things}},
	doi = {10.1109/JIOT.2015.2498900},
	abstract = {The Internet of Things (IoT) envisages a future in which digital and physical things or objects (e.g., smartphones, TVs, cars) can be connected by means of suitable information and communication technologies, to enable a range of applications and services. The IoT's characteristics, including an ultra-large-scale network of things, device and network level heterogeneity, and large numbers of events generated spontaneously by these things, will make development of the diverse applications and services a very challenging task. In general, middleware can ease a development process by integrating heterogeneous computing and communications devices, and supporting interoperability within the diverse applications and services. Recently, there have been a number of proposals for IoT middleware. These proposals mostly addressed wireless sensor networks (WSNs), a key component of IoT, but do not consider RF identification (RFID), machine-to-machine (M2M) communications, and supervisory control and data acquisition (SCADA), other three core elements in the IoT vision. In this paper, we outline a set of requirements for IoT middleware, and present a comprehensive review of the existing middleware solutions against those requirements. In addition, open research issues, challenges, and future research directions are highlighted.},
	number = {1},
	journal = {IEEE Internet of Things Journal},
	author = {Razzaque, M. A. and Milojevic-Jevric, M. and Palade, A. and Clarke, S.},
	month = feb,
	year = {2016},
	keywords = {communications devices, digital things, heterogeneous computing, information and communication technologies, Internet of things, Internet of Things, Internet of Things (IoT) characteristics, interoperability, IoT Characteristics, IoT middleware, M2M Communication, machine-to-machine (M2M) communication, middleware, Middleware, middleware requirements, Middleware Requirements, network level heterogeneity, physical things, Radiofrequency identification, Real-time systems, RF identification (RFID), RFID, SCADA, Security, supervisory control and data acquisition (SCADA), ultra-large-scale network of things, wireless sensor networks, Wireless sensor networks, wireless sensor networks (WSNs), WSN, WSNs},
	pages = {70--95},
	file = {IEEE Xplore Abstract Record:/home/luis/Zotero/storage/QM65EEVF/7322178.html:text/html;IEEE Xplore Full Text PDF:/home/luis/Zotero/storage/M5KZHAJL/Razzaque et al. - 2016 - Middleware for Internet of Things A Survey.pdf:application/pdf}
}

@misc{ETSIWelc14:online,
  author = {},
  title = {ETSI - Welcome to the World of Standards!},
  howpublished = {\url{http://www.etsi.org/}},
  month = {},
  year = {2018},
  note = {(Accessed on 01/26/2018)}
}

@misc{FIWARE27:online,
author = {},
title = {FIWARE},
howpublished = {\url{https://www.fiware.org/}},
month = {},
year = {2018},
note = {(Accessed on 01/26/2018)}
}

@misc{oneM2MHo30:online,
author = {},
title = {oneM2M - Home},
howpublished = {\url{http://www.onem2m.org/}},
month = {},
year = {2018},
note = {(Accessed on 02/02/2018)}
}


@inproceedings{dobbelaere_kafka_2017,
	address = {New York, NY, USA},
	series = {{DEBS} '17},
	title = {Kafka {Versus} {RabbitMQ}: {A} {Comparative} {Study} of {Two} {Industry} {Reference} {Publish}/{Subscribe} {Implementations}: {Industry} {Paper}},
	isbn = {978-1-4503-5065-5},
	shorttitle = {Kafka {Versus} {RabbitMQ}},
	url = {http://doi.acm.org/10.1145/3093742.3093908},
	doi = {10.1145/3093742.3093908},
	abstract = {Publish/subscribe is a distributed interaction paradigm well adapted to the deployment of scalable and loosely coupled systems. Apache Kafka and RabbitMQ are two popular open-source and commercially-supported pub/sub systems that have been around for almost a decade and have seen wide adoption. Given the popularity of these two systems and the fact that both are branded as pub/sub systems, two frequently asked questions in the relevant online forums are: how do they compare against each other and which one to use? In this paper, we frame the arguments in a holistic approach by establishing a common comparison framework based on the core functionalities of pub/sub systems. Using this framework, we then venture into a qualitative and quantitative (i.e. empirical) comparison of the common features of the two systems. Additionally, we also highlight the distinct features that each of these systems has. After enumerating a set of use cases that are best suited for RabbitMQ or Kafka, we try to guide the reader through a determination table to choose the best architecture given his/her particular set of requirements.},
	urldate = {2018-03-13},
	booktitle = {Proceedings of the 11th {ACM} {International} {Conference} on {Distributed} and {Event}-based {Systems}},
	publisher = {ACM},
	author = {Dobbelaere, Philippe and Esmaili, Kyumars Sheykh},
	year = {2017},
	keywords = {AMQP, Apache Kafka, Log Files, Message Brokers, Performance, Publish/Subscribe Systems, RabbitMQ, Reliability},
	pages = {227--238},
	file = {ACM Full Text PDF:/Users/rmorla/Documents/Work/references/zotero/zotero/storage/TA24D65V/Dobbelaere and Esmaili - 2017 - Kafka Versus RabbitMQ A Comparative Study of Two .pdf:application/pdf}
}

@article{fernandez-rodraguez_benchmarking_2017,
	title = {Benchmarking real-time vehicle data streaming models for a smart city},
	volume = {72},
	issn = {0306-4379},
	url = {http://www.sciencedirect.com/science/article/pii/S0306437917301916},
	doi = {https://doi.org/10.1016/j.is.2017.09.002},
	journal = {Information Systems},
	author = {Fernández-Rodríguez, Jorge Y. and Álvarez-García, Juan A. and Fisteus, Jesús Arias and Luaces, Miguel R. and Magaña, Victor Corcoba},
	year = {2017},
	keywords = {Big Data, Data streaming, Distributed systems, Simulator, Smart city},
	pages = {62 -- 76},
	file = {FernÃ¡ndez-RodrÃ­guez et al. - 2017 - Benchmarking real-time vehicle data streaming mode.pdf:/Users/rmorla/Documents/Work/references/zotero/zotero/storage/X8FHESEL/FernÃ¡ndez-RodrÃ­guez et al. - 2017 - Benchmarking real-time vehicle data streaming mode.pdf:application/pdf}
}

@inproceedings{zhang_psbench:_2014,
	address = {New York, NY, USA},
	series = {Middleware {Posters} and {Demos} '14},
	title = {{PSBench}: {A} {Benchmark} for {Content}- and {Topic}-based {Publish}/{Subscribe} {Systems}},
	isbn = {978-1-4503-3220-0},
	shorttitle = {{PSBench}},
	url = {http://doi.acm.org/10.1145/2678508.2678517},
	doi = {10.1145/2678508.2678517},
	abstract = {The publish/subscribe paradigm has found wide acceptance in a broad variety of use cases that differ dramatically in the characteristics of their workloads. Many different systems have been developed both by academia as well as industry, but there is no definitive benchmark, which enables a fair comparison between the different systems. In this demo, we present PSBench, a benchmark specification and suite for publish/subscribe systems that covers a broad variety of publish/subscribe workloads and scenarios. The benchmark suite is extensible and generic, but the specification targets social games. Social games are the ideal use case since they have a very broad range of requirements and produce a variety of publications and subscriptions. We draw from our experience in massive multiplayer online games to construct a highly realistic workload. In this demo, we present the toolchain, the workload and the graphical interfaces that enable an extensive performance evaluation of publish/subscribe systems.},
	urldate = {2018-03-13},
	booktitle = {Proceedings of the {Posters} \& {Demos} {Session}},
	publisher = {ACM},
	author = {Zhang, Kaiwen and Rabl, Tilmann and Sun, Yi Ping and Kumar, Rushab and Zen, Nayeem and Jacobsen, Hans-Arno},
	year = {2014},
	keywords = {benchmarking, content-based matching, publish/subscribe, topic-based matching},
	pages = {17--18},
	file = {ACM Full Text PDF:/Users/rmorla/Documents/Work/references/zotero/zotero/storage/JSYDTEK4/Zhang et al. - 2014 - PSBench A Benchmark for Content- and Topic-based .pdf:application/pdf}
}

@article{shukla_riotbench:_2017,
	title = {{RIoTBench}: {An} {IoT} benchmark for distributed stream processing systems},
	volume = {29},
	issn = {1532-0634},
	shorttitle = {{RIoTBench}},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/cpe.4257/abstract},
	doi = {10.1002/cpe.4257},
	abstract = {The Internet of Things (IoT) is an emerging technology paradigm where millions of sensors and actuators help monitor and manage physical, environmental, and human systems in real time. The inherent closed-loop responsiveness and decision making of IoT applications make them ideal candidates for using low latency and scalable stream processing platforms. Distributed stream processing systems (DSPS) hosted in cloud data centers are becoming the vital engine for real-time data processing and analytics in any IoT software architecture. But the efficacy and performance of contemporary DSPS have not been rigorously studied for IoT applications and data streams. Here, we propose RIoTBench, a real-time IoT benchmark suite, along with performance metrics, to evaluate DSPS for streaming IoT applications. The benchmark includes 27 common IoT tasks classified across various functional categories and implemented as modular microbenchmarks. Further, we define four IoT application benchmarks composed from these tasks based on common patterns of data preprocessing, statistical summarization, and predictive analytics that are intrinsic to the closed-loop IoT decision-making life cycle. These are coupled with four stream workloads sourced from real IoT observations on smart cities and smart health, with peak streams rates that range from 500 to 10 000messages/second from up to 3million sensors. We validate the RIoTBench suite for the popular Apache Storm DSPS on the Microsoft Azure public cloud and present empirical observations. This suite can be used by DSPS researchers for performance analysis and resource scheduling, by IoT practitioners to evaluate DSPS platforms, and even reused within IoT solutions.},
	language = {en},
	number = {21},
	urldate = {2018-03-13},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Shukla, Anshu and Chaturvedi, Shilpa and Simmhan, Yogesh},
	month = nov,
	year = {2017},
	keywords = {performance evaluation, Internet of Things, big data applications, benchmark, dataflows, distributed stream processing},
	pages = {n/a--n/a},
	file = {Shukla et al. - 2017 - RIoTBench An IoT benchmark for distributed stream.pdf:/Users/rmorla/Documents/Work/references/zotero/zotero/storage/2MS93V2V/Shukla et al. - 2017 - RIoTBench An IoT benchmark for distributed stream.pdf:application/pdf}
}

@incollection{iosup_iaas_2014,
	title = {{IaaS} {Cloud} {Benchmarking}: {Approaches}, {Challenges}, and {Experience}},
	isbn = {978-1-4939-1904-8 978-1-4939-1905-5},
	shorttitle = {{IaaS} {Cloud} {Benchmarking}},
	url = {https://link.springer.com/chapter/10.1007/978-1-4939-1905-5_4},
	abstract = {Infrastructure-as-a-Service (IaaS) cloud computing is an emerging commercial infrastructure paradigm under which clients (users) can lease resources when and for how long needed, under a cost model that reflects the actual usage of resources by the client. For IaaS clouds to become mainstream technology and for current cost models to become more clientfriendly, benchmarking and comparing the non-functional system properties of various IaaS clouds is important, especially for the cloud users. In this article we focus on the IaaS cloud-specific elements of benchmarking, from a userâ€™s perspective. We propose a generic approach for IaaS cloud benchmarking, discuss numerous challenges in developing this approach, and summarize our experience towards benchmarking IaaS clouds. We argue for an experimental approach that requires, among others, new techniques for experiment compression, new benchmarking methods that go beyond blackbox and isolated-user testing, new benchmark designs that are domain-specific, and new metrics for elasticity and variability.},
	language = {en},
	urldate = {2018-03-13},
	booktitle = {Cloud {Computing} for {Data}-{Intensive} {Applications}},
	publisher = {Springer, New York, NY},
	author = {Iosup, Alexandru and Prodan, Radu and Epema, Dick},
	year = {2014},
	doi = {10.1007/978-1-4939-1905-5_4},
	pages = {83--104},
	file = {Full Text PDF:/Users/rmorla/Documents/Work/references/zotero/zotero/storage/6XVJWCTK/Iosup et al. - 2014 - IaaS Cloud Benchmarking Approaches, Challenges, a.pdf:application/pdf;Snapshot:/Users/rmorla/Documents/Work/references/zotero/zotero/storage/8WA3QW24/978-1-4939-1905-5_4.html:text/html}
}

@inproceedings{varghese_doclite:_2016,
	title = {{DocLite}: {A} {Docker}-{Based} {Lightweight} {Cloud} {Benchmarking} {Tool}},
	shorttitle = {{DocLite}},
	doi = {10.1109/CCGrid.2016.14},
	abstract = {Existing benchmarking methods are time consuming processes as they typically benchmark the entire Virtual Machine (VM) in order to generate accurate performance data, making them less suitable for real-time analytics. The research in this paper is aimed to surmount the above challenge by presenting DocLite - Docker Container-based Lightweight benchmarking tool. DocLite explores lightweight cloud benchmarking methods for rapidly executing benchmarks in near real-time. DocLite is built on the Docker container technology, which allows a user-defined memory size and number of CPU cores of the VM to be benchmarked. The tool incorporates two benchmarking methods - the first referred to as the native method employs containers to benchmark a small portion of the VM and generate performance ranks, and the second uses historic benchmark data along with the native method as a hybrid to generate VM ranks. The proposed methods are evaluated on three use-cases and are observed to be up to 91 times faster than benchmarking the entire VM. In both methods, small containers provide the same quality of rankings as a large container. The native method generates ranks with over 90\% and 86\% accuracy for sequential and parallel execution of an application compared against benchmarking the whole VM. The hybrid method did not improve the quality of the rankings significantly.},
	booktitle = {2016 16th {IEEE}/{ACM} {International} {Symposium} on {Cluster}, {Cloud} and {Grid} {Computing} ({CCGrid})},
	author = {Varghese, B. and Subba, L. T. and Thai, L. and Barker, A.},
	month = may,
	year = {2016},
	keywords = {Benchmark testing, cloud benchmarking, cloud computing, Cloud computing, containers, Containers, Docker, docker container, DocLite, hybrid benchmark, Hybrid power systems, lightweight benchmark, lightweight cloud benchmarking tool, Memory management, Random access memory, Real-time systems, storage management, user-defined memory size, virtual machine, virtual machines},
	pages = {213--222},
	file = {IEEE Xplore Abstract Record:/Users/rmorla/Documents/Work/references/zotero/zotero/storage/79WWRJWW/7515691.html:text/html;IEEE Xplore Full Text PDF:/Users/rmorla/Documents/Work/references/zotero/zotero/storage/I3VT5K77/Varghese et al. - 2016 - DocLite A Docker-Based Lightweight Cloud Benchmar.pdf:application/pdf}
}